## ğŸ“ Repository Contents

### 1. `Copy_of_EDA_+_BEST_MODEL_ONLY.ipynb`
- **Target Variable:** `log_trip_duration`
- Applies a log transformation to improve distribution and model performance.
- Produces the **best fit model** with reduced **MAPE** and minimal underprediction bias.
- Recommended notebook for **final model deployment**.

### 2. `Another_copy_of_EDA_+_BEST_MODEL_ONLY(With_TRIP_DURN).ipynb`
- **Target Variable:** `trip_duration` (original scale in seconds).
- Offers good performance but shows a **slightly higher percentage error**.
- Helpful for interpretation in real-world time units.

### 3. `First_EXECUTION.ipynb`
- Contains **initial EDA**, **baseline models**, and **feature engineering trials**.
- Useful for understanding the **modeling journey and experimentation**.
---

Feel free to fork, star â­, and con

---
# ğŸ—½ NYC Taxi Trip Duration Prediction ğŸš•

A machine learning capstone project focused on predicting taxi trip durations in New York City using historical trip data, regression models, and hyperparameter tuning.

---

## ğŸ“Œ Problem Statement

The objective is to predict the **trip duration** of NYC taxi rides using a combination of temporal, spatial, and vendor-related features. Accurate predictions help:
- Improve ETA predictions for users
- Optimize dispatch systems
- Reduce wait time, operational cost, and customer dissatisfaction

---

## ğŸ“Š Dataset

Data Source: [Kaggle - NYC Taxi Trip Duration](https://www.kaggle.com/competitions/nyc-taxi-trip-duration)

- Rows: ~55 million
- Features:
  - Pickup/Dropoff timestamps and locations
  - Passenger count
  - Distance (Haversine)
  - Vendor ID
  - Day of the week, hour, month
  - Store-and-forward flag

---

## ğŸ”§ Feature Engineering

- âœ… Extracted `pickup_hour`, `pickup_month`, `pickup_day` from datetime  
- âœ… Calculated `distance_km` using the Haversine formula  
- âœ… Applied **encoding** to categorical features  
- âœ… Created binary features (e.g. `store_and_fwd_flag_Y`)  
- âœ… Transformed `trip_duration` using log for some models

---

## ğŸ¤– Models Trained

| Model             | Regularization | Notes                              |
|------------------|----------------|-------------------------------------|
| Linear Regression| âŒ             | Baseline                           |
| Ridge Regression | âœ… L2          | With cross-validation              |
| Lasso Regression | âœ… L1          | With `GridSearchCV`                |
| Random Forest    | âŒ             | Ensemble trees                     |
| XGBoost          | âœ…             | Tuned with **Optuna**, best model  |
| KNN Regressor    | âŒ             | For neighborhood comparison        |

---

## ğŸ§ª Evaluation Metrics

Metrics selected for business impact and interpretability:
- **MAE**: Lower values reduce ETA errors
- **RMSE**: Penalizes large errors (good for long trips)
- **RÂ² Score**: Measures variance explained
- **Adjusted RÂ²**: Controls for feature count
- **MAPE**: Scaled % error (business friendly)
- **Bias**: Mean % error, shows under/overestimation

---

## ğŸ” Hyperparameter Optimization

- `Lasso` & `Ridge`: `GridSearchCV`
- `XGBoost`: Tuned using **Optuna**

**Best XGBoost Parameters:**
- `n_estimators`: 222
- `max_depth`: 6
- `learning_rate`: 0.146
- `subsample`: 0.962
- `colsample_bytree`: 0.937

---

## ğŸ“ˆ Final Results

| Model        | MAE  | RMSE | RÂ²    |
|--------------|------|------|-------|
| Ridge        | 4.21 | 5.89 | 0.58  |
| RandomForest | 3.91 | 5.23 | 0.65  |
| **XGBoost**  | 3.35 | 4.63 | 0.72  |

---

### âœ… Final XGBoost Model Performance:

Training RÂ² : 0.8173
Test RÂ² : 0.8035
MAE (Test) : 0.2272
RMSE (Test) : 0.3226
Adjusted RÂ² (Test) : 0.8035
Mean Absolute % Error : 3.73%
Mean % Error (Bias) : -0.34%

---


---

## ğŸ“Š Visualizations

- âœ… Actual vs Predicted Line Plot  
- âœ… Feature Importance using SHAP  
- âœ… Trip Duration Histogram  
- âœ… Distance vs Duration correlation  
- âœ… Model Score Comparison Chart  

---
